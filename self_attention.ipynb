{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self-attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTDwgZxtKpWK",
        "outputId": "1c5b8287-b39d-4057-e7f4-e5005392f7f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### In this exercise, you will implement a self-attention mechanism\n",
        "### for a toy spam classification task.\n",
        "### Given a sentence [w1, w2, w3], we'll perform self-attention and then \n",
        "### aggregate the representations for feeding to a classifier.\n",
        "### We provide the data / training loop setup; you should def look it over and \n",
        "### understand how it works!!\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "\n",
        "# spam detection!\n",
        "data = ['you won a billion dollars , great work !',\n",
        "        'click here for cs685 midterm answers',\n",
        "       'read important cs685 news',\n",
        "       'send me your bank account info asap']\n",
        "\n",
        "labels = torch.LongTensor([1, 1, 0, 1]) # store ground-truth labels\n",
        "\n",
        "# let's do some preprocessing\n",
        "vocab = {}\n",
        "inputs = []\n",
        "\n",
        "for sent in data:\n",
        "    idxes = []\n",
        "    sent = sent.split()\n",
        "    for w in sent:\n",
        "        if w not in vocab:\n",
        "            vocab[w] = len(vocab)\n",
        "        idxes.append(vocab[w])\n",
        "    inputs.append(idxes)\n",
        "    \n",
        "print(inputs)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14], [15, 16, 12, 17], [18, 19, 20, 21, 22, 23, 24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFBSGlNQLDdo",
        "outputId": "365fa0ea-18b8-42b5-929b-9c777a3c9a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "class SelfAttentionNN(nn.Module):\n",
        "    \n",
        "    # this is where you initialize network parameters\n",
        "    def __init__(self, d_emb, num_outputs, len_vocab):\n",
        "        \n",
        "        super(SelfAttentionNN, self).__init__()\n",
        "        self.d_emb = d_emb\n",
        "        self.embeddings = nn.Embedding(len_vocab, d_emb)\n",
        "        self.Wq = nn.Linear(d_emb, d_emb) # project to query space\n",
        "        self.Wk = nn.Linear(d_emb, d_emb) # project to keys\n",
        "        self.Wv = nn.Linear(d_emb, d_emb) # project to values\n",
        "        self.output = nn.Linear(d_emb, num_outputs) # output matrix before softmax\n",
        "        \n",
        "    # all three args are N x d_emb matrices\n",
        "    def dot_product_attn(self, q, k, v):\n",
        "        scores = torch.mm(q, k.t()) # gets all dot products at once, N X N\n",
        "        scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "        return torch.mm(scores, v) # N x d_emb\n",
        "    \n",
        "    def bilinear_attn(self, q, k):\n",
        "        pass\n",
        "    \n",
        "    def scaled_dot_product_attn(self, q, k):\n",
        "        scores = torch.mm(q, k.t()) # gets all dot products at once, N X N\n",
        "        scores = scores / q.size()[1]\n",
        "        scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "        return torch.mm(scores, q) # N x d_emb\n",
        "    \n",
        "    def mlp_attn(self, q, k):\n",
        "        pass\n",
        "        \n",
        "    def forward(self, input):\n",
        "        N = input.size()[1] # number of tokens in input, assume N > 2\n",
        "        embs = self.embeddings(input).squeeze(0) # N x d_emb\n",
        "        \n",
        "        attn_reps = torch.zeros(embs.size()) # store attn weighted average of each position in seq\n",
        "                                             # N x d_emb\n",
        "        \n",
        "        queries = self.Wq(embs) # N x d_emb\n",
        "        keys = self.Wk(embs) # N x d_emb\n",
        "        values = self.Wv(embs)\n",
        "\n",
        "        # simple slow solution for self-attn computation\n",
        "        for i in range(N):\n",
        "\n",
        "            # at each position i, take query q_i and dot product w/ all keys\n",
        "            unnorm_scores = torch.mv(keys, queries[i])\n",
        "            probs = torch.nn.functional.softmax(unnorm_scores, dim=0)\n",
        "            \n",
        "            # attn-weighted average of value vectors\n",
        "            # ave_values = torch.mv(values.t(), probs)\n",
        "            ave_values = torch.sum(values * probs[:, None], dim=0) # (optional) [:, None] adds a dimension at index 1\n",
        "\n",
        "            attn_reps[i] = ave_values\n",
        "\n",
        "        # compose attn_reps into a single vector\n",
        "        attn_reps = torch.mean(attn_reps, dim=0)\n",
        "\n",
        "        # efficient attention computation using dot product attention\n",
        "        attn_reps_2 = self.dot_product_attn(queries, keys, values)\n",
        "        attn_reps_2 = torch.mean(attn_reps_2, dim=0)\n",
        "\n",
        "        # efficient attention computation using scaled dot product attention\n",
        "        attn_reps_3 = self.scaled_dot_product_attn(queries, keys)\n",
        "        attn_reps_3 = torch.mean(attn_reps_3, dim=0)\n",
        "\n",
        "        pred = self.output(attn_reps_2) # return logits\n",
        "        return pred.unsqueeze(0)\n",
        "\n",
        "# hyperparameters\n",
        "num_epochs = 10\n",
        "net = SelfAttentionNN(20, 2, len(vocab))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.SGD(net.parameters(), lr = 0.1)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs): # how many passes thru input im gonna do\n",
        "    ep_loss = 0.\n",
        "        \n",
        "    # for each pass, loop over training data\n",
        "    for i in range(len(inputs)):\n",
        "        sent = torch.LongTensor(inputs[i]).unsqueeze(0) # pick a sentence\n",
        "        target = labels[i].unsqueeze(0) # get its label\n",
        "        # unsqueeze(0) shapes the tensors into 2d, as pytorch forward usually\n",
        "        # is written to receive minibatches rather than single examples\n",
        "        \n",
        "        pred = net(sent) # get output of network (unnormalized logits)\n",
        "        loss = loss_fn(pred, target) # apply softmax + cross entropy\n",
        "        loss.backward() # do backprop!!!\n",
        "        optim.step() # update parameters w/ gradient descent\n",
        "        optim.zero_grad() # reset gradients for next example\n",
        "        ep_loss += loss # update overall loss for monitoring progress\n",
        "    \n",
        "    print(epoch, ep_loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(2.9852, grad_fn=<AddBackward0>)\n",
            "1 tensor(1.5828, grad_fn=<AddBackward0>)\n",
            "2 tensor(0.8187, grad_fn=<AddBackward0>)\n",
            "3 tensor(0.4224, grad_fn=<AddBackward0>)\n",
            "4 tensor(0.2354, grad_fn=<AddBackward0>)\n",
            "5 tensor(0.1514, grad_fn=<AddBackward0>)\n",
            "6 tensor(0.1071, grad_fn=<AddBackward0>)\n",
            "7 tensor(0.0807, grad_fn=<AddBackward0>)\n",
            "8 tensor(0.0636, grad_fn=<AddBackward0>)\n",
            "9 tensor(0.0517, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHA-OKlGLKI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}