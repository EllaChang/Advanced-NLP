{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"implementing_NLM_with_attention.ipynb","provenance":[{"file_id":"1cRCtU1OQb22vhtdykNIDWyr5tGFlbjMv","timestamp":1603668308362},{"file_id":"1sCPcnty03LnfrEvJUQGZS1u0vm0K5S3a","timestamp":1569884404873}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zK3cUsqPwsMl"},"source":["# Homework 2, CS585 Fall 2019 (due Oct 18th 2019)\n","\n","### **This HW is due on Oct 18th, 2019, submitted via Gradescope as a PDF (File > Print > Save as PDF). 100 points total.**\n","\n","#### IMPORTANT: After copying this notebook to your Google Drive, please add a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n","LINK: *paste your link here*\n","\n","##### How to do this problem set:\n","\n","- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out all code-blocks that say `IMPLEMENT ME`.\n","\n","- For all of the textual answers you have to fill out have placeholder text which says  `Answer in one or two sentences here.` For each question, you need to replace `Answer in one or two sentences here` with your answer.\n"," \n","- The neural language model may take up to 10 minutes to train, so **start early**! The rest of the cells are designed so that you can run in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code.\n","\n","##### How to submit this problem set:\n","- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Print -> Save as PDF) and upload to Gradescope.\n","  \n","- **Important:** check your PDF before you turn it in to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n","\n","- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n","\n","##### Academic honesty \n","\n","- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n","\n","- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n","\n","To get started, first run the following cell to create a PyDrive client and download data to your own Google Drive."]},{"cell_type":"code","metadata":{"id":"5Cz0lbXL2_19","executionInfo":{"status":"ok","timestamp":1569937882354,"user_tz":240,"elapsed":9454,"user":{"displayName":"Simeng Sun","photoUrl":"","userId":"16312449915221158433"}},"outputId":"c50692e6-228f-4a96-baa8-f58f91460a20","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","print('success!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["success!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8bZDdek4mo2"},"source":["import torch, pickle, os, sys, random, time\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch import nn, optim\n","from collections import *\n","import numpy as np\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVKsHApr4zkD"},"source":["# Download wikitext02 data\n","data = {'test': '', 'train': '', 'valid': ''}\n","f_id = {'wiki.test.tokens': '1T8yMDqCqrWVXYJiTqKe2q8hoyAD_rV84', \n","        'wiki.train.tokens': '1itIse_TpmFq1I5y7mxuXtzljnRwoM8gF', \n","        'wiki.valid.tokens': '1K3SV5p8gu19DgQZtRvCViGMb26NJ1scd'}\n","\n","for fname in f_id:\n","  f = drive.CreateFile({'id' : f_id[fname]})\n","  f.GetContentFile(fname)\n","  with open(fname, 'r') as f_wiki:\n","    data[fname.split('.')[1]] = f_wiki.read().lower().split()\n","\n","vocab = list(set(data['train']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzRJcts-xirF"},"source":["Now have a look at the data by running the below cell."]},{"cell_type":"code","metadata":{"id":"GLE1v7mRIKt-","executionInfo":{"status":"ok","timestamp":1569937885358,"user_tz":240,"elapsed":12399,"user":{"displayName":"Simeng Sun","photoUrl":"","userId":"16312449915221158433"}},"outputId":"e3ca93b8-3321-445a-a8db-52b2ea5a5680","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["print('train : %s ...' % data['train'][:10])\n","print('dev : %s ...' % data['valid'][:10])\n","print('test : %s ...' % data['test'][:10])\n","print('first 10 words in vocab: %s' % vocab[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senj≈ç', 'no', 'valkyria', '3', ':'] ...\n","dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n","test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n","first 10 words in vocab: ['badai', 'mates', 'gauge', 'frankish', 'principality', 'pathologist', 'professionally', 'interactions', 'peziza', 'rooted']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3TZ91yDrl1Oq"},"source":["## Q1. N-gram Language model (40pts)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qhEuT7w5ClUg"},"source":["\n","### **Q1.1:** Train N-gram language model (15pts)\n","\n","Complete the following *train_ngram_lm* function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n","\n","*Input:*\n","+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n","+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model) If order=3, we compute $p(w_2 | w_0, w_1)$.\n","\n","*Output:*\n","+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n","\n","Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n","\n","    \n","    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n","    \n","In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n","\n","*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."]},{"cell_type":"code","metadata":{"id":"S13ismv99-N1"},"source":["def train_ngram_lm(data, order=3):\n","    \"\"\"\n","        Train n-gram language model\n","    \"\"\"\n","    \n","    # pad (order-1) special tokens to the left\n","    # for the first token in the text\n","    order -= 1\n","    data = ['<S>'] * order + data # \n","    lm = defaultdict(Counter)\n","    for i in range(len(data) - order):\n","        \"\"\"\n","        IMPLEMENT ME!\n","        \n","        \"\"\"  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK0QYLxd49x3"},"source":["def test_ngram_lm():\n","  \n","    print('checking empty history ...')\n","    lm1 = train_ngram_lm(data['train'], order=1)\n","    assert '' in lm1, \"empty history should be in the language model!\"\n","    \n","    print('checking probability distributions ...')\n","    lm2 = train_ngram_lm(data['train'], order=2)\n","    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n","    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n","    \n","    print('checking lengths of histories ...')\n","    lm3 = train_ngram_lm(data['train'], order=3)\n","    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n","    \n","    print('checking word distribution values ...')\n","    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n","           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n","           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n","           \"values do not match!\"\n","    \n","    print(\"Congratulations, you passed the ngram check!\")\n","    \n","  \n","test_ngram_lm()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EjKT7pwJE6WY"},"source":["### **Q1.2:** Generate text from n-gram language model (10pts)\n","\n","Complete the following *generate_text* function based on these input/output requirements:\n","\n","*Input:*\n","\n","+ **lm**: the lm object, a dictionary you return from  the **train_ngram_lm** function\n","+ **vocab**: vocab is a list of unique word types in the training set computed already computed for you during data loading.\n","+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n","+ **order**: order of your language model (i.e., \"n\" in the n-gram model)\n","+ **num_tok**: number of tokens to be generated following the input context\n","\n","\n","*Output:*\n","\n","+ generated text, should be a space-separated string\n","    \n","*Hint:*\n","\n","After getting the next-word distribution given history, try using **[numpy.random.choice](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.choice.html)** to sample the next word from the distribution."]},{"cell_type":"code","metadata":{"id":"jx0BFoF1E5dF"},"source":["# generate text\n","def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n","    \n","    # The goal is to generate new words following the context\n","    # If context has more tokens than the order of lm, \n","    # generate text that follows the last (order-1) tokens of the context\n","    # and store it in the variable `history`\n","    order -= 1\n","    history = context.split()[-order:]\n","    # `out` is the list of tokens of context\n","    # you need to append the generated tokens to this list\n","    out = context.split()\n","        \n","    for i in range(num_tok):\n","        \"\"\"\n","        IMPLEMENT ME!\n","        \n","        \"\"\"\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qPPhLK3HF5L"},"source":["Now try to generate some texts! Read the texts generated by ngram language model with different orders"]},{"cell_type":"code","metadata":{"id":"BSlNevanHIlM"},"source":["order = 1\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ibmvkwl9HMzd"},"source":["order = 2\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQNB3FqKHibm"},"source":["order = 3\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSO4l5z7HjGU"},"source":["order = 4\n","generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLovkCIrHy0H"},"source":["### Q1.3 : Complete *compute_perplexity* (15pts)\n","Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n","\n","Recall perplexity is the inverse probability of the test text\n","$$\\text{ppl}(w_1, \\dots, w_n) = p(w_1, \\dots, w_n)^{-\\frac{1}{N}}$$\n","\n","For an n-gram model, perplexity is computed by\n","$$\\text{ppl}(w_1, \\dots, w_n) = (\\prod_i p(w_{i+n}|w_i^{i+n-1})^{-\\frac{1}{N}}$$\n","\n","To get rid of numerical issue, we usually compute through:\n","$$\\text{ppl}(w_1, \\dots, w_n) = \\exp(-\\frac{1}{N}\\sum_i \\log p(w_{i+n}|w_i^{i+n-1}))$$\n","\n","\n","*Input:*\n","\n","+ **lm**: the language model you trained, still the object you returned from the **train_ngram_lm** function\n","+ **data**: test data\n","+ **vocab**: vocab\n","+ **order**: order of the lm\n","\n","*Output:*\n","\n","+ the perplexity of test data\n","\n","*Hint:*\n","\n","+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."]},{"cell_type":"code","metadata":{"id":"FKi4AczgHj1t"},"source":["def compute_perplexity(lm, data, vocab, order=3):\n","    \n","    # pad according to order\n","    order -= 1\n","    data = ['<S>'] * order + data\n","    for i in range(len(data) - order):\n","        h, w = ' '.join(data[i: i+order]), data[i+order]\n","        \"\"\"\n","        IMPLEMENT ME!\n","        # if h not in lm, back-off to n-1 gram and look up again\n","\n","        \"\"\"\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hVpItwZhI6ac"},"source":["Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, 4-gram language model should be around 795, 203, 141, 130 respectively."]},{"cell_type":"code","metadata":{"id":"SpN70HA2H9C-"},"source":["for o in [1, 2, 3, 4]:\n","    lm = train_ngram_lm(data['train'], order=o)\n","    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9tRCKZ5BJJOV"},"source":["## Q2. Neural language models (60pts)\n"]},{"cell_type":"markdown","metadata":{"id":"5jlXIjm6WZBE"},"source":["In this part of the homework, we'll be using PyTorch to play around with neural language models. First, a quick warm up by implementing backpropagation within a *scalar* neural network. Then, you'll implement a neural language model using PyTorch's built-in modules.\n","\n","Firstly, run the cell below to import pytorch and set up the gradient checking functionality."]},{"cell_type":"code","metadata":{"id":"VOhQHHAPV6LD"},"source":["import torch\n","import torch.nn as nn\n","device = torch.device('cpu')\n","\n","# checks equality between your gradients and those from autograd\n","def gradient_check(params, your_gradient):\n","    all_good = True\n","    for key in params.keys():\n","        if params[key].grad.size() != your_gradient[key].size():\n","            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n","                % (key, your_gradient[key].size(), \n","                   params[key].grad.size()))\n","            all_good = False\n","        elif not torch.allclose(params[key].grad, your_gradient[key], atol=1e-6):\n","            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n","                % (key, your_gradient[key].detach(), \n","                   params[key].grad))\n","            all_good = False\n","            \n","    return all_good"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kt0UNgpWuJ6"},"source":["### Q2.1 Warm up with single neuron (5 pts)\n","The below cell trains a network with scalars (i.e., single neurons) in each layer on a small dataset of ten examples. We saw a similar architecture in the class notes. All you have to do is translate the partial derivatives we computed into code. The network is defined as:\n","\n","<center>$\\text{h} = \\tanh(w_1 \\cdot \\text{input})$</center>\n","\n","<center>$\\text{pred} = \\tanh(w_2 \\cdot \\text{h})$</center>\n","\n","<center>$\\text{L} = 0.5 \\cdot (\\text{target} - \\text{pred})^2$</center>\n","\n","If you run the cell below, you should see \"GRADIENT ERRORS\". Once you implement the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ correctly, you will instead see a \"SUCCESS\" message. **Do NOT modify any code outside of the block marked \"IMPLEMENT BACKPROP HERE\"!**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"HfLgwTNYWGt3"},"source":["# initialize model parameters\n","params = {}\n","params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n","params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n","\n","# set up some training data\n","inputs = torch.randn(20, 1)\n","targets = inputs / 2\n","\n","# training loop\n","all_good = True\n","for i in range(len(inputs)):\n","    \n","    ## forward prop, then compute loss.\n","    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n","    hidden = torch.tanh(a)\n","    b = params['w2'] * hidden\n","    pred = torch.tanh(b)\n","    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n","    loss.backward() # runs autograd\n","    \n","    \n","    ####################\n","    # TODO: IMPLEMENT BACKPROP HERE\n","    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n","    your_gradient = {}\n","    your_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n","    your_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n","    \n","    # IMEPLEMENT ME!\n","    \n","    # END \n","    ####################\n","    \n","    if not gradient_check(params, your_gradient):\n","        all_good = False\n","        break\n","    \n","    # zero gradients after each training example\n","    params['w1'].grad.zero_()\n","    params['w2'].grad.zero_() \n","    \n","if all_good:\n","    print('SUCCESS! you passed the gradient check.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5bzxVSAOPUn"},"source":["### Q2.2 RNN language model (10 pts)\n","\n","For this part of the homework, we will use **PyTorch** to build our model. The below cell preprocesses the raw text so you can load it directly. The input to your model is a *minibatch* of sequences which takes the form of a  $N \\times L$ matrix  where $N$ is the batch size and $L$ is the maximum sequence length. For each minibatch, your models should produce an $N \\times L \\times V$ tensor where $V$ is the size of the vocabulary. This tensor stores the predicted probability distribution of the next word for every position of every sequence in the batch. Note that each batch is padded to dimensionality $L=40$ using the special padding token <*pad>*; similarly, each sequence begins with the <*bos>* token and ends with the <*eos>* token. Please look at the PyTorch RNN documentation if you're having problems getting started: https://pytorch.org/docs/stable/nn.html#rnn\n","\n","Firstly run the cell below to download the data. **If you see \"device: cpu\", please change your Colab runtime to the GPU backend by going to \"Runtime > Change runtime type > Hardware accelerator > GPU\"**"]},{"cell_type":"code","metadata":{"id":"xtOZWDtTSCAG"},"source":["import torch, pickle, os, sys, random, time\n","from torch import nn, optim\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print('device: ', device)\n","\n","# Download id2word\n","f_wikitext = drive.CreateFile({'id': '1fBS7PyEOeQMuH5Ea1_hnEjU3PmFE7ZZc'})\n","f_wikitext.GetContentFile('./wikitext.pkl') \n","with open('./wikitext.pkl', 'rb') as f_in:\n","  wikitext = pickle.load(f_in)\n","  \n","wikitext['train'] = torch.LongTensor(wikitext['train']).to(device)\n","wikitext['dev'] = torch.LongTensor(wikitext['valid']).to(device)\n","wikitext['test'] = torch.LongTensor(wikitext['test']).to(device)\n","idx_to_word = wikitext['id2word']\n","word_to_idx = {idx_to_word[k]: k for k in idx_to_word}\n","\n","\n","print(\"Wikitext data downloaded!\")\n","# Demonstrate id2word\n","print('There are ' + str(len(idx_to_word)) + ' words in vocabulary')\n","for id in range(8):\n","  print('Word id ' + str(id) + \" stands for '\" + str(idx_to_word[id]) + \"\\'\")\n","print('...')\n","print((wikitext['train'] > 0).sum())\n","    \n","print('Set up finished')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_3C-13G_m2Q"},"source":["The following cell contains code for computing perplexity and training the neural language model. Run the cell, and please make sure you (at least roughly) understand what is happening, but **do not modify any part of it**."]},{"cell_type":"code","metadata":{"id":"onfjIrblON0g"},"source":["# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n","def compute_perplexity(dataset, net, bsz=64):\n","    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n","    num_examples, seq_len = dataset.size()\n","    \n","    # we'll still use batches b/c we can't fit the whole\n","    # validation set into GPU memory\n","    batches = [(start, start + bsz) for start in\\\n","               range(0, num_examples, bsz)]\n","    \n","    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n","    nll = 0.\n","    for b_idx, (start, end) in enumerate(batches):\n","            \n","        batch = dataset[start:end]\n","        ut = torch.nonzero(batch).size(0)\n","        preds = net(batch)\n","        targets = batch[:, 1:].contiguous().view(-1)\n","        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n","        loss = criterion(preds, targets)\n","        nll += loss.detach()\n","        total_unmasked_tokens += ut\n","\n","    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n","    return perplexity.data\n","    \n","\n","# training loop for language models, DO NOT MODIFY!\n","def train_lm(dataset, params, net):\n","    \n","    # since the first index corresponds to the PAD token, we just ignore it\n","    # when computing the loss\n","    criterion = nn.CrossEntropyLoss(ignore_index=0)\n","    \n","    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n","    num_examples, seq_len = dataset.size()    \n","    batches = [(start, start + params['batch_size']) for start in\\\n","               range(0, num_examples, params['batch_size'])]\n","    \n","    for epoch in range(params['epochs']):\n","        ep_loss = 0.\n","        start_time = time.time()\n","        random.shuffle(batches)\n","        net.train()\n","        # for each batch, calculate loss and optimize model parameters            \n","        for b_idx, (start, end) in enumerate(batches):\n","                        \n","            batch = dataset[start:end]\n","            preds = net(batch)\n","\n","            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n","            # q1.1: explain the below line!\n","            targets = batch[:, 1:].contiguous().view(-1)\n","            loss = criterion(preds, targets)\n","            \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(net.parameters(), 3)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            ep_loss += loss\n","        \n","        net.eval()\n","        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\\\n","              (epoch, ep_loss, time.time()-start_time, compute_perplexity(wikitext['dev'], net)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVwFC8cHLWye"},"source":["Now implement the following class, which defines a recurrent neural language model, by filling in the __forward__ function."]},{"cell_type":"code","metadata":{"id":"IBC5pTTgIBVH"},"source":["class RNNLM(nn.Module):\n","    def __init__(self, params):\n","        super(RNNLM, self).__init__()\n","        self.vocab_size = params['vocab_size']\n","        self.d_emb = params['d_emb']\n","        self.d_hid = params['d_hid']\n","        self.n_layer = 1\n","        self.batch_size = params['batch_size']\n","        \n","        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n","        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n","        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n","        \n","    def forward(self, batch):\n","        \"\"\"\n","            IMPLEMENT ME!\n","            Encode the data using the embedding layer you initialized.\n","            Pass the encoded data and hidden states to your RNN.\n","            Return unnormalized logits for each token's prediction.\n","            \n","            Why just logits? Check the document of torch.nn.CrossEntropyLoss,\n","            since it combines nn.LogSoftmax() and nn.NLLLoss(), \n","            you don't need to explicitly use the softmax function!\n","        \"\"\"\n","        batch_size, seq_len= batch.shape\n","        hidden = (torch.zeros(self.n_layer, batch_size, self.d_hid).to(device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfzBU3SCWIso"},"source":["Run the following cell to test that your implementation is at least returning tensors of the proper dimensionality. Note that this is just a sanity check to help you develop. Your RNNLM might still be implemented incorrectly even if it passes. You will have to obtain a reasonable perplexity after training on WikiText to be certain that you've done it right."]},{"cell_type":"code","metadata":{"id":"wv3NEVi7AaCE"},"source":["def test_RNNLM():\n","    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n","    params = {}\n","    params['vocab_size'] = len(idx_to_word)\n","    params['d_emb'] = 8\n","    params['d_hid'] = 8\n","    params['batch_size'] = 5\n","    testnet = RNNLM(params)\n","    testnet.to(device)\n","    test_output = testnet(test_batch)\n","    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n","                                                          (params['batch_size'], test_output.shape[0])\n","    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n","                                                          (test_batch.shape[1], test_output.shape[1])\n","    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n","                                                          (params['vocab_size'], test_output.shape[2])\n","    print(\"Congratulations, you passed the RNNLM test!\")\n","test_RNNLM()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7li14NHkLimS"},"source":["Once you pass the above test, train your RNNLM model on WikiText by running the below cell. It should take a couple minutes per epoch."]},{"cell_type":"code","metadata":{"id":"tGvKkmqMET1w"},"source":["# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n","params = {}\n","params['vocab_size'] = len(idx_to_word)\n","params['d_emb'] = 512\n","params['d_hid'] = 256\n","params['batch_size'] = 64\n","params['epochs'] = 5\n","params['learning_rate'] = 0.001\n","\n","RNNnet = RNNLM(params)\n","RNNnet.to(device)\n","train_lm(wikitext['train'], params, RNNnet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwNvc39bLnqY"},"source":["After training is finished, run the below cell to get the perplexity on the test set. If you did it right, your perplexity should be around 135-140."]},{"cell_type":"code","metadata":{"id":"xD6NYw62St2J"},"source":["RNNnet.eval() # we're no longer training the network\n","print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], RNNnet)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DqYr-aIXUsU-"},"source":["### Q2.3 Explain the code (5 pts)\n","\n","These lines in the provided **compute_perplexity** function: \n","\n","```\n","            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n","            targets = batch[:, 1:].contiguous().view(-1)\n","            loss = criterion(preds, targets)\n","```\n","\n","Look at the documentation of the CrossEntropyLoss criterion [here](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Then, explain what these lines accomplish by answering the following question:\n","\n","*   What does the indexing in front of *batch* accomplish in the second line? Why can't we just use the *batch* variable as the target? (5 pts)\n","\n","  * *Answer in one or two sentences here*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lKu_fZJ6VUL8"},"source":["### Q2.4 Neural Language Model with attention (25 pts)\n","\n","Only start working on this after you've correctly implemented the RNNLM in the previous problem, as you'll want to copy over some code here. \n","Complete the foward function of both the **ATTNLM** and **Attention** modules by following the instructions in the comment block. **Each epoch may take 3-5 minutes to run, so start early!**"]},{"cell_type":"code","metadata":{"id":"Ja339lSzVajG"},"source":["# An RNN language model with attention, you implement this!\n","class ATTNLM(nn.Module):\n","    def __init__(self, params):\n","        super(ATTNLM, self).__init__()\n","        \n","        self.vocab_size = params['vocab_size']\n","        self.d_emb = params['d_emb']\n","        self.d_hid = params['d_hid']\n","        self.n_layer = 1\n","        self.btz = params['batch_size']\n","        \n","        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n","        self.attn = Attention(self.d_hid)\n","        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n","        # the combined_W maps the combined hidden states and context vectors to d_hid \n","        self.combined_W = nn.Linear(self.d_hid * 2, self.d_hid)\n","        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n","        \n","\n","    def forward(self, batch, return_attn_weights=False):\n","        \n","        \"\"\"\n","            IMPLEMENT ME!\n","            Copy your implementation of RNNLM, make sure it passes the RNNLM check\n","            In addition to that, you need to add the following 3 things\n","            1. pass rnn output to attention module, get context vectors and attention weights\n","            2. concatenate the context vec and rnn output, pass the combined\n","               vector to the layer dealing with the combined vectors (self.combined_W)\n","            3. if return_attn_weights, instead of return the [N, L, V]\n","               matrix, return the attention weight matrix\n","               of dimension [N, L, L] which returned from the forrward function of Attnetion module\n","        \"\"\"\n","        batch_size, seq_len= batch.shape\n","        hidden = torch.zeros(self.n_layer, batch_size, self.d_hid).to(device)\n","                \n","        \n","class Attention(nn.Module):\n","    def __init__(self, d_hidden):\n","        super(Attention, self).__init__()\n","        self.linear_w1 = nn.Linear(d_hidden, d_hidden)\n","        self.linear_w2 = nn.Linear(d_hidden, 1)\n","        \n","    \n","    def forward(self, x):\n","      \n","        \"\"\"\n","            IMPLEMENT ME!\n","            For each time step t\n","                1. Obtain attention scores for step 0 to (t-1)\n","                   This should be a dot product between current hidden state (x[:,t:t+1,:])\n","                   and all previous states x[:, :t, :]. While t=0, since there is not\n","                   previous context, the context vector and attention weights should be of zeros.\n","                   You might find torch.bmm useful for computing over the whole batch.\n","                2. Turn the scores you get for 0 to (t-1) steps to a distribution.\n","                   You might find F.softmax to be helpful.\n","                3. Obtain the sum of hidden states weighted by the attention distribution\n","            Concat the context vector you get in step 3. to a matrix.\n","            \n","            Also remember to store the attention weights, the attention matrix \n","            for each training instance should be a lower triangular matrix. Specifically,\n","            each row, element 0 to t-1 should sum to 1, the rest should be padded with 0.\n","            e.g. \n","            [ [0.0000, 0.0000, 0.0000, 0.0000],\n","              [1.0000, 0.0000, 0.0000, 0.0000],\n","              [0.4246, 0.5754, 0.0000, 0.0000],\n","              [0.2798, 0.3792, 0.3409, 0.0000] ]\n","            \n","            Return the context vector matrix and the attention weight matrix\n","            \n","        \"\"\"\n","        batch_seq_len = x.shape[1]        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqyjOwGz8Y6b"},"source":["Run the following cell to sanity check your implementation; do not continue until you pass all of the tests!"]},{"cell_type":"code","metadata":{"id":"_0-MHHC2KYv8"},"source":["def test_ATTNLM():\n","    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n","    params = {}\n","    params['vocab_size'] = len(idx_to_word)\n","    params['d_emb'] = 8\n","    params['d_hid'] = 8\n","    params['batch_size'] = 5\n","    testnet = ATTNLM(params)\n","    testnet.to(device)\n","    test_output = testnet(test_batch)\n","    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n","                                                          (params['batch_size'], test_output.shape[0])\n","    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n","                                                          (test_batch.shape[1], test_output.shape[1])\n","    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n","                                                          (params['vocab_size'], test_output.shape[2])\n","    testnet = ATTNLM(params)\n","    testnet.to(device)\n","    test_output = testnet(test_batch, return_attn_weights=True)\n","    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n","                                                          (params['batch_size'], test_output.shape[0])\n","    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n","                                                          (test_batch.shape[1], test_output.shape[1])\n","    assert test_output.shape[2] == test_batch.shape[1], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n","                                                          (test_batch.shape[1], test_output.shape[2])\n","    prob_dist = torch.sum(test_output, dim=2)[:, 1:]\n","    assert all([x > 0.99 and x < 1.01 for x in prob_dist.reshape(-1)]), \"attention weights not properly normalized, got {}\".format(prob_dist)\n","    print(\"Congratulations, you passed the ATTNLM test!\")\n","\n","test_ATTNLM()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRxeoJW3Ltzk"},"source":["Now, train your ATTNLM model on WikiText by running the below cell. If the perplexity on dev set is `nan` or `inf`, it is likely the model has corrupted due to gradient exploding/vanishing or other numerical instability issue, stop this cell and run it again."]},{"cell_type":"code","metadata":{"id":"81LKL_7pKYAC"},"source":["# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n","params = {}\n","params['vocab_size'] = len(idx_to_word)\n","params['d_emb'] = 512\n","params['d_hid'] = 256\n","params['n_layer'] = 1\n","params['batch_size'] = 64\n","params['epochs'] = 6\n","params['learning_rate'] = 0.0005\n","\n","ATTNnet = ATTNLM(params)\n","ATTNnet.cuda()\n","train_lm(wikitext['train'], params, ATTNnet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENa03ZlJLyFF"},"source":["Finally, compute the perplexity on the test set. If you implemented it correctly, you should get a perplexity of around 145-150. Due to random effects, it is possible to get perplexity slightly lower than 145. Make sure you didn't add any additional nonlinearity operation which can lead to lower perplexity."]},{"cell_type":"code","metadata":{"id":"LoOvc6quW0Ef"},"source":["ATTNnet.eval() # we're no longer training the network\n","print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], ATTNnet)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eSJQCTtCgcb"},"source":["### Q2.5 Explain the results (5 pts)\n","\n","If you implemented this correctly, you should notice a similar (or slightly higher) perplexity for the ATTNLM than for RNNLM. Considering attention is supposed to be a net improvement for RNN-type models, give two possible reasons why the ATTNLM's perplexity isn't significantly lower.\n","\n","  * *Answer in **2-4** sentences here*"]},{"cell_type":"markdown","metadata":{"id":"TCyBN6KtEpLy"},"source":["### Q2.6 Generate text from the neural LMs (5 pts)\n","Run the below cell to generate some text from your RNNLM and ATTNLM."]},{"cell_type":"code","metadata":{"id":"fkzzhEYCCY5A"},"source":["def sample_from_lm(net, context, max_words=50):\n","  \n","    with torch.no_grad():\n","        for i in range(max_words):\n","            data = torch.LongTensor([context]).to(device)\n","            decoded = net(data)\n","            decoded = decoded[0, -1].exp().cpu()\n","            w_i = torch.multinomial(decoded, 1)[0].item()\n","            if w_i in [1, 2, 3]:\n","                continue\n","            context.append(w_i)\n","\n","        return context\n","\n","word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n","context = [word_to_idx[w] for w in 'he is the '.split()]\n","\n","rnn_completion = sample_from_lm(RNNnet, context)\n","print('rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYg9ki0dYoyi"},"source":["word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n","context = [word_to_idx[w] for w in 'he is the '.split()]\n","\n","rnn_completion = sample_from_lm(ATTNnet, context)\n","print('attention rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KU63vLoNE76l"},"source":["Do you notice any differences in coherence or grammaticality compared to the n-gram models? What about any differences between the RNNLM and the ATTNLM? If you observed any distinct differences, explain why you think they exist; if not, explain why all of the outputs appear to be of similar quality. \n","  * *Answer in **2-4** sentences here*"]},{"cell_type":"markdown","metadata":{"id":"xBGJPkitfizQ"},"source":["### Q2.7 Interpreting attention (5 pts)\n","Finally, let's visualize some attention heatmaps by running the below two cells. "]},{"cell_type":"code","metadata":{"id":"jXykMRedFfE2"},"source":["def plot_attn_heatmap(sent):\n","  \n","    sent_in_id = [word_to_idx[w] for w in sent.split()]\n","\n","    with torch.no_grad():\n","        data = torch.LongTensor([sent_in_id]).to(device)\n","        weights = ATTNnet(data, return_attn_weights=True)\n","    \n","    fig, ax = plt.subplots()\n","\n","    sent_sp = sent.split()\n","    ax.set_xticks(np.arange(len(sent_sp)))\n","    ax.set_yticks(np.arange(len(sent_sp)))\n","    ax.set_xticklabels(sent_sp)\n","    ax.set_yticklabels(sent_sp)\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n","\n","    plt.imshow(weights[0, :].cpu())\n","\n","sent = \"top warning signs earth is warming , according to experts\"\n","plot_attn_heatmap(sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szDBrOdpFflZ"},"source":["sent = \"us cities lose 36 million trees each year . here is why it matters \"\n","plot_attn_heatmap(sent)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGh2jq9tG50n"},"source":["Each row of these plots represents the attention weights on the history tokens when the model is trying to predict the next word. For example, the third row of the first plot can be interpreted as the attention weights over \"top\" and \"warning\" when predicting \"signs\"; you'll note that the rest of the row is black (i.e., zero attention on future words). Are these attention maps interpretable? If you (as a human) were solving the same word prediction problem, would you focus on the same words as the ATTNLM does?\n","\n","  * *Answer in **2-4** sentences here*\n"]},{"cell_type":"code","metadata":{"id":"6EhxYhx2HyPY"},"source":[""],"execution_count":null,"outputs":[]}]}