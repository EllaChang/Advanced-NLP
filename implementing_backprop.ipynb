{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"implementing_backprop.ipynb","provenance":[{"file_id":"1ouK0POKEYu8RiCiv-2mcs45Zxzdfbp0D","timestamp":1603668400202},{"file_id":"1Hg0Ekz-_7jQ_4w2IQxg8RS6NBXRcUgsK","timestamp":1549393569449},{"file_id":"1GFgM_ZnrQryCQSzBpKEVyXiIMwv5UIip","timestamp":1549173722336},{"file_id":"1-FMuvqRKe-RpSFrSKcim90L7emdss8Er","timestamp":1549173709286}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3LsFKULpuoTz"},"source":["## CS690D: Homework 1 (due 2/19/19 on Gradescope)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wTOfXSMCvlps"},"source":["This homework contains both a written component and an implementation component. First, you'll answer a couple of questions about word embeddings (problems 4 and 5 from Eisenstein ch. 14).  Please format any equations or math in your answers using LaTeX; see the equations in the problem statements for examples. \n","\n","### 1.1 (10 pts): \n","A simple way to compute a distributed phrase representation is to add up the distributed representations of the words in the phrase. Consider a sentiment analysis\n","model in which the predicted sentiment is, $\\psi(w) = \\theta · (\\sum_{m=1}^M x_m)$, where $x_m$ is\n","the vector representation of word $m$. Prove that in such a model, the following two\n","inequalities cannot both hold:\n","\n","<center>$\\psi(\\text{good}) >\\psi(\\text{not good})$</center>\n","<center>$\\psi(\\text{bad}) <\\psi(\\text{not bad})$</center>"]},{"cell_type":"markdown","metadata":{"id":"wGKZpkBgxatc"},"source":["#### write your answer here!!!"]},{"cell_type":"markdown","metadata":{"id":"T5QSXLPXxg4A"},"source":["### 1.2(10 pts)\n","Now let’s consider a slight modification to the prediction model in the previous\n","problem:\n","\n","<center>$\\psi(w) = \\theta \\cdot \\text{ReLu}( \\sum_{m=1}^M x_m)$</center>\n","\n","Show that in this case, it is possible to achieve the inequalities above. Your solution\n","should provide the weights $\\theta$ and the embeddings $x_\\text{good}, x_\\text{bad},$ and $x_\\text{not}$."]},{"cell_type":"markdown","metadata":{"id":"d3YyWOMuy7nJ"},"source":["#### write your answer here!!!"]},{"cell_type":"markdown","metadata":{"id":"3wQ0Tt4UvhdF"},"source":["### PART 2: IMPLEMENTING BACKPROP\n","\n","In the remaining three problems, you will implement *backpropagation*  to train several different neural architectures. Specifically, your job is to compute the partial derivatives of the provided loss function with respect to each parameter of the network. Your work will be checked automatically against the output of pytorch's autograd. We will provide partial credit for each problem in the event that you are able to correctly compute some partial derivatives but not others.\n","\n","**WARNINGS: DO NOT USE OUTPUT FROM AUTOGRAD IN YOUR COMPUTATIONS. DO NOT MODIFY ANY CODE OUTSIDE OF THE SPECIFIED BLOCKS WHERE YOU ARE TO IMPLEMENT BACKPROP. IF OUR GRADING SCRIPTS DETECT ANY VIOLATIONS, YOU WILL RECEIVE ZERO POINTS FOR THAT PROBLEM.**\n","\n","Run the below cell to import pytorch and set up the gradient checking functionality.\n"]},{"cell_type":"code","metadata":{"id":"b4Q2X-BB7YZk"},"source":["import torch\n","import torch.nn as nn\n","device = torch.device('cpu')\n","\n","# checks equality between your gradients and those from autograd\n","# 'params' and 'your_gradient' are both dictionaries with the same keys, the names of parameters.\n","# 'params' contains model parameters augmented with pytorch's automatically-computed gradients.  \n","# 'your_gradient' contains tensors you calculated yourself.  This function assumes pytorch computed\n","# them correctly, and checks whether your calculations match.\n","def gradient_check(params, your_gradient):\n","    all_good = True\n","    for key in params.keys():\n","        if params[key].grad.size() != your_gradient[key].size():\n","            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n","                % (key, your_gradient[key].size(), \n","                   params[key].grad.size()))\n","            all_good = False\n","        elif not torch.allclose(params[key].grad, your_gradient[key], atol=1e-6):\n","            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n","                % (key, your_gradient[key].detach(), \n","                   params[key].grad))\n","            all_good = False\n","            \n","    return all_good"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxCa23c_7asB"},"source":["\n","### 2.1: Warmup with single neurons (20 points)\n","The below cell trains a network with just single neurons in each layer on a small dataset of ten examples. We saw this same network architecture in class, so all you have to do is translate the partial derivatives we computed into code. As a reminder, the network is defined as:\n","\n","<center>$\\text{h} = \\tanh(w_1 * \\text{input})$</center>\n","\n","<center>$\\text{pred} = \\tanh(w_2 * \\text{h})$</center>\n","\n","If you run the cell below, you should see \"GRADIENT ERRORS\". Once you implement the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ correctly, you will instead see a \"SUCCESS\" message. \n"]},{"cell_type":"code","metadata":{"id":"V6c-2vlxut1a"},"source":["# initialize model parameters\n","params = {}\n","params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n","params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n","\n","# set up some training data\n","inputs = torch.randn(20, 1)\n","targets = inputs / 2\n","\n","# training loop\n","all_good = True\n","for i in range(len(inputs)):\n","    \n","    ## forward prop, then compute loss.\n","    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n","    hidden = torch.tanh(a)\n","    b = params['w2'] * hidden\n","    pred = torch.tanh(b)\n","    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n","    loss.backward() # runs autograd\n","    \n","    ####################\n","    # TODO: IMPLEMENT BACKPROP HERE\n","    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n","    your_gradient = {}\n","    your_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n","    your_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n","    # END \n","    ####################\n","\n","    if not gradient_check(params, your_gradient):\n","        all_good = False\n","        break\n","    \n","    # zero gradients after each training example\n","    params['w1'].grad.zero_()\n","    params['w2'].grad.zero_() \n","    \n","if all_good:\n","    print('SUCCESS! you passed the gradient check.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqlJWq5sf1al"},"source":["### 2.2: Deep averaging network (30 points)\n","Now we're going to move to a more complex network featuring multiple matrix/vector operations. Instead of taking single numbers as input, this network takes in *a single sequence of word embeddings* associated with a sentence. We'll call the input $X$; it has dimensionality $N \\times D$, where $N$ is the number of words in the sentence and $D$ is the word embedding dimensionality. We'll denote the $i^{th}$ word embedding in the sentence, which corresponds to the $i^{th}$ row of $X$, as $X_i$. The network is trained using softmax / cross entropy loss for sentiment analysis, so each input is associated with a target value $t$ (positive, negative, or neutral). The network is described by the following set of equations:\n","\n","<center>$\\text{ave} = \\frac{1}{N} \\sum_{i=0}^{N} X_i$</center>\n","\n","<center>$\\text{h} = \\text{ReLu}(W_1 \\cdot \\text{ave})$</center>\n","\n","<center>$\\text{pred} = \\text{softmax}(W_2 \\cdot h)$</center>\n","\n","where $\\text{ReLu}(x) = max(0, x)$. We have provided you with the $\\text{softmax}$ derivative, which you will use to compute three partial derivatives: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, and $\\frac{\\partial{L}}{\\partial{X}}$. \n","\n","Just as in the previous problem when you run the cell below, you should see \"GRADIENT ERRORS\", which will be replaced with a \"SUCCESS\" message once the gradient has been correctly implemented."]},{"cell_type":"code","metadata":{"id":"Z7YbZFwMwsgA"},"source":["# let's set some hyperparameters first\n","N = 4 # all sentences will be of length 4\n","D = 5 # word embedding dim = 5\n","M = 3 # hidden dimensionality\n","labels = {'negative':0, 'neutral':1, 'positive':2}\n","vocab = {'really':0, 'movie':1, 'was':2, 'good':3, 'not':4, 'okay':5}\n","num_labels = len(labels)\n","len_vocab = len(vocab)\n","\n","# initialize model parameters\n","params = {}\n","params['X'] = torch.randn(len_vocab, D, requires_grad=True)\n","params['W1'] = torch.randn(M, D, requires_grad=True) \n","params['W2'] = torch.randn(num_labels, M, requires_grad=True) \n","\n","# set up some training data\n","inputs = [('positive', 'movie was really good'),\n","          ('neutral', 'really really really okay'),\n","          ('negative', 'movie was not good')]\n","\n","# training loop\n","all_good = True\n","for i in range(len(inputs)):\n","    \n","    # obtain word embeddings for input sentence\n","    target, sentence = inputs[i]\n","    target = labels[target]\n","    input = torch.LongTensor(N)\n","    for j, w in enumerate(sentence.split()):\n","        input[j] = vocab[w]\n","    \n","    ## forward prop, then compute loss.\n","    ave = torch.zeros(D) # first let's compute the word embedding average\n","    for j in range(N):\n","        w_idx = input[j]\n","        ave += params['X'][w_idx]\n","    ave = ave / N\n","        \n","    # now we'll pass it through the DAN\n","    a = torch.mv(params['W1'], ave)\n","    h = torch.relu(a)\n","    b = torch.mv(params['W2'], h)\n","    pred = torch.softmax(b, 0)\n","    \n","    loss = -1 * torch.log(pred[target]) # negative log likelihood of target class\n","    loss.backward() # runs autograd\n","    \n","    # we give you the derivative for the softmax / CE loss as \"dLdb\"\n","    dLdb = pred.clone().detach()\n","    dLdb[target] -= 1.\n","    \n","    ####################\n","    # TODO: IMPLEMENT BACKPROP HERE\n","    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n","    your_gradient = {}\n","    your_gradient['W2'] = torch.zeros(params['W2'].size()) # implement dL/dW2\n","    your_gradient['W1'] = torch.zeros(params['W1'].size()) # implement dL/dW1\n","    your_gradient['X'] = torch.zeros(params['X'].size()) # implement dL/dX\n","    # END\n","    ####################\n","    \n","    if not gradient_check(params, your_gradient):\n","        all_good = False\n","        break\n","    \n","    # zero gradients after each training example\n","    params['X'].grad.zero_()\n","    params['W1'].grad.zero_()\n","    params['W2'].grad.zero_() \n","    \n","if all_good:\n","    print('SUCCESS! you passed the gradient check.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0IEmz-08F-O"},"source":["### 2.3: Recurrent neural network (30 points)\n","For the last (and most difficult) problem of this homework, we'll implement *backpropagation through time*,  an extension of backpropagation for recurrent neural networks. We'll stick with the same sentiment analysis example as before but change the network architecture from a DAN to a variant of an RNN with a multiplicative term. Remember that computations in an RNN proceed sequentially, so for inputs where $N=4$, we compute four different hidden states ($h_0, h_1, h_2, h_3$) and feed the final hidden state $h_3$ to the output layer. The network is then defined as:\n","\n","<center>$h_i= \\tanh(W_h \\cdot h_{i-1} + W_x \\cdot X_i + h_{i-1} * h_{i-2}$)</center>\n","<center>$\\text{pred} = \\text{softmax}(W_\\text{out} \\cdot h_3)$</center>\n","\n","You'll need to correctly compute four partial derivatives to get the SUCCESS message: $\\frac{\\partial{L}}{\\partial{W_h}}$, $\\frac{\\partial{L}}{\\partial{W_x}}$, $\\frac{\\partial{L}}{\\partial{W_\\text{out}}}$,and $\\frac{\\partial{L}}{\\partial{X}}$. \n","\n","##### Hint: It might be helpful to think of an RNN as a feed-forward network by \"unrolling\" it over the time dimension. "]},{"cell_type":"code","metadata":{"id":"ANVEjk1pvYvw"},"source":["# initialize model parameters\n","params = {}\n","params['X'] = torch.randn(len_vocab, D, requires_grad=True)\n","params['Wx'] = torch.randn(M, D, requires_grad=True) \n","params['Wh'] = torch.randn(M, M, requires_grad=True)\n","params['Wout'] = torch.randn(num_labels, M, requires_grad=True) \n","\n","# training loop\n","all_good = True\n","for i in range(len(inputs)):\n","    \n","    # obtain word embeddings for input sentence\n","    target, sentence = inputs[i]\n","    target = labels[target]\n","    input = torch.LongTensor(N)\n","    for j, w in enumerate(sentence.split()):\n","        input[j] = vocab[w]\n","    \n","    ## forward prop, then compute loss.\n","    hiddens = {} # stores hidden state / intermediate vars at each timestep\n","    for j in range(N):\n","        w_idx = input[j]\n","        hiddens[j] = {}\n","        \n","        # no previous hidden state, just project word embedding\n","        if j == 0:\n","            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx]) \n","            \n","        elif j == 1:\n","            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx]) + \\\n","                torch.mv(params['Wh'], hiddens[j-1]['h'])\n","            \n","        else:\n","            hiddens[j]['a'] = torch.mv(params['Wx'], params['X'][w_idx]) + \\\n","                torch.mv(params['Wh'], hiddens[j-1]['h']) + \\\n","                hiddens[j-1]['h'] * hiddens[j-2]['h']\n","            \n","        hiddens[j]['h'] = torch.tanh(hiddens[j]['a'])\n","\n","    b = torch.mv(params['Wout'], hiddens[N-1]['h'])\n","    pred = torch.softmax(b, 0)\n","    \n","    loss = -1 * torch.log(pred[target]) # negative log likelihood of target class\n","    loss.backward() # runs autograd\n","    \n","    # we give you the derivative for the softmax / CE loss as \"dLdb\"\n","    dLdb = pred.clone().detach()\n","    dLdb[target] -= 1.\n","    \n","    ####################\n","    # TODO: IMPLEMENT BACKPROP HERE\n","    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n","    your_gradient = {}\n","    your_gradient['Wout'] = torch.zeros(params['Wout'].size()) # implement dL/dWout\n","    your_gradient['X'] = torch.zeros(params['X'].size()) # implement dL/dX\n","    your_gradient['Wx'] = torch.zeros(params['Wx'].size()) # implement dL/dWx\n","    your_gradient['Wh'] = torch.zeros(params['Wh'].size()) # implement dL/dWh\n","    # END\n","    ####################\n","            \n","    if not gradient_check(params, your_gradient):\n","        all_good = False\n","        break\n","    \n","    # zero gradients after each training example\n","    params['X'].grad.zero_()\n","    params['Wx'].grad.zero_()\n","    params['Wh'].grad.zero_() \n","    params['Wout'].grad.zero_() \n","\n","    \n","if all_good:\n","    print('SUCCESS! you passed the gradient check.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8JTEmr--TJV"},"source":["# $"],"execution_count":null,"outputs":[]}]}